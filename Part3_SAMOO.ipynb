{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984af293",
   "metadata": {},
   "source": [
    "# Speeding Up the Search for \"Compromise Solutions\" Before It's Too Late\n",
    "### A walk through on Surrogate-Assisted MOO (SAMOO)\n",
    "\n",
    "Now that we know Pareto optimality and that there's a tool to efficiently perform MOO, we have to face another problem: such undertaking is computationally expensive -- for large-scale real-world numerical model, one such forward run could take hours, let alone hundreds of thousand of runs required in MOO.\n",
    "\n",
    "For this tutorial, we will demonstrate how we can speed up the process of MOO by using a surrogate model, without compromising the reliability of solutions. We will still be using the Fonseca-Fleming problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da493aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197eecfb",
   "metadata": {},
   "source": [
    "Recall that the Fonseca-Fleming problem has two objectives with different decision variable values for their respective minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonseca_fleming(x):\n",
    "    x = np.array(x)\n",
    "    n = len(x)\n",
    "    # Fonseca-Fleming objective functions\n",
    "    # f1(x) = 1 - exp(-sum((xi - 1/sqrt(n))^2))\n",
    "    # f2(x) = 1 - exp(-sum((xi + 1/sqrt(n))^2))\n",
    "    term1 = np.sum((x - 1/np.sqrt(n))**2)\n",
    "    term2 = np.sum((x + 1/np.sqrt(n))**2)\n",
    "    \n",
    "    obj1 = 1 - np.exp(-term1)\n",
    "    obj2 = 1 - np.exp(-term2)\n",
    "    \n",
    "    return obj1, obj2\n",
    "\n",
    "n_samples = 1000\n",
    "x1_range = np.linspace(-4, 4, int(np.sqrt(n_samples)))\n",
    "x2_range = np.linspace(-4, 4, int(np.sqrt(n_samples)))\n",
    "x1, x2 = np.meshgrid(x1_range, x2_range)\n",
    "x1 = x1.flatten()\n",
    "x2 = x2.flatten()\n",
    "\n",
    "x1 = x1[:n_samples]\n",
    "x2 = x2[:n_samples]\n",
    "\n",
    "n_actual = min(len(x1), len(x2))\n",
    "objectives = np.array([fonseca_fleming([x1[i], x2[i]]) for i in range(n_actual)])\n",
    "obj1 = objectives[:, 0]\n",
    "obj2 = objectives[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbfbc2",
   "metadata": {},
   "source": [
    "Let us first generate the template files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_d = \"./base_files\"\n",
    "assert os.path.exists(base_d)\n",
    "\n",
    "temp_d = \"fonseca_fleming_demo\"\n",
    "if os.path.exists(temp_d):\n",
    "    shutil.rmtree(temp_d)\n",
    "shutil.copytree(base_d,temp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b6006",
   "metadata": {},
   "source": [
    "We can easily find Pareto optimal set of decision variables by running PESTPP-MOU. Let's generate an initial swarm size of 30 and perform 20 iterations of MOO. To ensure that our initial population is evenly distributed and sufficiently samples the decision space, it is advisable to use Latin Hypercube Sampling (LHS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831677f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the size of initial sample or the number of inner iterations if you want to try other values. \n",
    "# Just be aware that larger sample size and more inner iterations will take longer time to run.\n",
    "swarm_size = 30\n",
    "nmax_inner = 20\n",
    "\n",
    "sys.path.append(temp_d)\n",
    "from LHS_sampler import generate_lhsstarter\n",
    "bounds = np.array([[-4, 4] for i in range(2)])\n",
    "generate_lhsstarter(os.path.join(temp_d,'fon_pbm_template'), seed=42, n_samples=swarm_size, n_dimensions=2, bounds=bounds) \n",
    "\n",
    "pst_file = os.path.join(temp_d, 'fon_pbm_template', 'fon.pst')\n",
    "pst = pyemu.Pst(pst_file)\n",
    "pst.pestpp_options['mou_population_size'] = swarm_size\n",
    "pst.pestpp_options['mou_dv_population_file'] = 'starter.dv_pop.csv'\n",
    "pst.control_data.noptmax = nmax_inner\n",
    "pst.write(os.path.join(temp_d, 'fon_pbm_template', os.path.basename(pst_file)))\n",
    "\n",
    "starter_pop = pd.read_csv(os.path.join(temp_d, \"fon_pbm_template\", \"starter.dv_pop.csv\"), index_col=\"real_name\")\n",
    "starter_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f05f9",
   "metadata": {},
   "source": [
    "Now that we have generated an initial population, let us run pestpp-mou..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fefa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "tmpl_in = os.path.join(temp_d, \"fon_pbm_template\")\n",
    "sys.path.insert(0,tmpl_in)\n",
    "from forward_pbrun import ppw_worker as ppw_function\n",
    "pyemu.os_utils.start_workers(tmpl_in, \"pestpp-mou\", \"fon.pst\", num_workers = num_workers,\n",
    "                             worker_root = temp_d, master_dir = os.path.join(temp_d, \"pbm_run\"),\n",
    "                             ppw_function = ppw_function)\n",
    "sys.path.remove(tmpl_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131c5af",
   "metadata": {},
   "source": [
    "Let's plot the resulting Pareto front. Use the slider to observe how the swarm's position evolves in the objective space at each iteration as it searches for the Pareto front until it eventually converge. Also observe how the swarm moves in the decision space.\n",
    "\n",
    "We will revisit this later to compare with SAMOO, which will be introduced next. This will be referred to as Complex MOO run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, HBox, VBox, Output\n",
    "from IPython.display import display\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "run_data = pd.read_csv(os.path.join(temp_d, \"pbm_run\", \"fon.pareto.summary.csv\"))\n",
    "max_gen = max(run_data['generation'])\n",
    "\n",
    "csvfiles = sorted(glob.glob(os.path.join(temp_d, \"pbm_run\", \"*[0-999].dv_pop.csv\"), recursive=True), \n",
    "                  key=lambda x: int(x.split(\".dv\")[0].split(\".\")[1]))\n",
    "all_dv_list = []\n",
    "for file in csvfiles:\n",
    "    generation = int(file.split(\".dv\")[0].split(\".\")[1])\n",
    "    df = pd.read_csv(file).assign(generation=generation)\n",
    "    df = df[['generation'] + [col for col in df.columns if col != 'generation']] \n",
    "    all_dv_list.append(df)\n",
    "all_dv = pd.concat(all_dv_list, ignore_index=True)\n",
    "\n",
    "csvfiles = sorted(glob.glob(os.path.join(temp_d, \"pbm_run\", \"*[0-999].obs_pop.csv\"), recursive=True), \n",
    "                      key=lambda x: int(x.split(\".obs\")[0].split(\".\")[1]))\n",
    "\n",
    "all_obs_list = []\n",
    "for file in csvfiles:\n",
    "    generation = int(file.split(\".obs\")[0].split(\".\")[1])\n",
    "    df = pd.read_csv(file).assign(generation=generation)\n",
    "    df = df[['generation'] + [col for col in df.columns if col != 'generation']] \n",
    "    all_obs_list.append(df)\n",
    "all_obs = pd.concat(all_obs_list, ignore_index=True)\n",
    "\n",
    "all_data = pd.merge(all_dv, all_obs, on=['generation', 'real_name'])\n",
    "max_gen = max(all_data['generation'])\n",
    "\n",
    "x = all_data['x1'].values\n",
    "y = all_data['x2'].values\n",
    "z1 = all_data['obj1'].values\n",
    "z2 = all_data['obj2'].values\n",
    "\n",
    "xi = np.linspace(min(x), max(x), 100)\n",
    "yi = np.linspace(min(y), max(y), 100)\n",
    "xi_grid, yi_grid = np.meshgrid(xi, yi)\n",
    "\n",
    "zi1 = griddata((x, y), z1, (xi_grid, yi_grid), method='cubic')\n",
    "zi2 = griddata((x, y), z2, (xi_grid, yi_grid), method='cubic')\n",
    "\n",
    "out_pareto = Output()\n",
    "out_obj_space = Output()\n",
    "out_contour = Output()\n",
    "\n",
    "true_pareto = pd.read_csv(os.path.join('fonseca_fleming_demo', 'fonseca_fleming_solution_obj.csv'))\n",
    "\n",
    "def moo_plot_all(generation):\n",
    "\n",
    "    with out_pareto:\n",
    "        out_pareto.clear_output(wait=True)\n",
    "        pareto = run_data.loc[(run_data['generation']==generation) & (run_data['nsga2_front'] == 1)]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(pareto['obj1'], pareto['obj2'], c='firebrick', s=50, alpha=0.7, label = 'current non-dominated positions')\n",
    "        plt.scatter(true_pareto['obj1'], true_pareto['obj2'], c='deepskyblue', s=20, zorder = -10, label='Pareto optimal solutions')\n",
    "        plt.xlabel('Objective 1')\n",
    "        plt.ylabel('Objective 2')\n",
    "        plt.legend()\n",
    "        plt.title(f'Pareto Front at Generation {generation}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    with out_obj_space:\n",
    "        out_obj_space.clear_output(wait=True)\n",
    "        all_points = run_data.loc[(run_data['generation']==generation)]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(all_points['obj1'], all_points['obj2'], edgecolor='green', c = 'none', s=50, alpha=0.7, label = 'all candidate positions')\n",
    "        plt.scatter(true_pareto['obj1'], true_pareto['obj2'], c='deepskyblue', s=20, zorder = -10, label='Pareto optimal solutions')\n",
    "        plt.xlabel('Objective 1')\n",
    "        plt.ylabel('Objective 2')\n",
    "        plt.legend()\n",
    "        plt.title(f'Objective Space at Generation {generation}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    with out_contour:\n",
    "        out_contour.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        contour1 = ax.contourf(xi_grid, yi_grid, zi1, 15, cmap='plasma', alpha=0.5)\n",
    "        fig.colorbar(contour1, ax=ax, label='Objective 1')\n",
    "        contour2 = ax.contour(xi_grid, yi_grid, zi2, 15, cmap='viridis', alpha=0.7)\n",
    "        fig.colorbar(contour2, ax=ax, label='Objective 2')\n",
    "\n",
    "        pareto_members = pareto['member'].values\n",
    "        pareto_dv = all_data[all_data['real_name'].isin(pareto_members)]\n",
    "        ax.scatter(pareto_dv['x1'], pareto_dv['x2'], edgecolor='firebrick', facecolor='none', s=40, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_title(f'Pareto Optimal Decisions at Generation {generation}')\n",
    "        ax.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "generation_slider = IntSlider(min=0, max=max_gen, step=1, value=max_gen, \n",
    "                             description='Generation:',\n",
    "                             continuous_update=False)\n",
    "moo_plot_all(max_gen)\n",
    "\n",
    "generation_slider.observe(lambda change: moo_plot_all(change['new']), names='value')\n",
    "display(VBox([\n",
    "    generation_slider,\n",
    "    HBox([out_obj_space, out_pareto]),\n",
    "    out_contour\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff5d40",
   "metadata": {},
   "source": [
    "As this benchmark is really easy, it doesn't take much time to finish the entire optimization process and it doesn't take a lot of iteration to obtain a set of Pareto optimal solutions. However, in reality, real-world models do not run this fast. Even with parallelisation, it can take a few hundred, even thousand iterations until the swarm converges to the Pareto front. If important timely decisions rely on the outcome of such optimisation process, we would need some help to speed up this process. \n",
    "\n",
    "Let's pretend for a while that the Fonseca-Fleming problem is a complex model that is expensive to evaluate. There's nothing we could do about the run time of the complex model. What we can do though is to employ an \"emulator\", a faster and cheaper model that approximates the relationships between the known (i.e., previously evaluated) input and outputs of the complex model in order to predict its response to some input values that were not previously evaluated. This emulator is also known as the surrogate model. Because the surrogate model is an approximation, its prediction has errors. These errors could mislead the decision-making to some suboptimal solutions that are not actually in the Pareto front. This is some price to pay for speeding up the process, but don't worry as we have means to manage these uncertainties and still obtain truly Pareto optimal solutions.\n",
    "\n",
    "There are many surrogate models available in literature. However, we will use Gaussian Process Regression (GPR) as it provides a convenient way to take into account the uncertainty in predictions of the surrogate model, which, as previously said, needs to be managed well.\n",
    "\n",
    "Surrogate-Assisted Multi-Objective Optimisation (SAMOO) follows this general algorithm:\n",
    "1. LHS sampling of initial training dataset and starting population\n",
    "2. Complex model evaluation (hereinafter referred to as Outer Iteration)\n",
    "3. Pareto dominance evaluation\n",
    "     - if front converged: exit; else: continue to step 4\n",
    "4. (Re)training the GPR\n",
    "5. MOO with GPR replacing the complex model (hereinafter referred to as Inner Iterations)\n",
    "6. Resampling for new training points (also called infills)\n",
    "\n",
    "Steps 2-6 are performed iteratively until convergence is achieved in Step 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c395ec8",
   "metadata": {},
   "source": [
    "To facilitate this entire process, we will use samoo_utils.py. Let us first prepare our PEST files (control, template, instruction files) to be SAMOO-ready. Let's create a new directory where we will put all our SAMOO-related files and generate an initial training dataset for the surrogate model using Latin hypercube sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d9969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(temp_d, 'samoo_runs')):\n",
    "    shutil.rmtree(os.path.join(temp_d, 'samoo_runs'))\n",
    "os.makedirs(os.path.join(temp_d, 'samoo_runs'))\n",
    "shutil.copytree(os.path.join(temp_d, 'fon_samoo_template'), os.path.join(temp_d, 'samoo_runs', 'template'))\n",
    "\n",
    "sys.path.insert(0,temp_d)\n",
    "from samoo_utils import SAMOO\n",
    "optimizer = SAMOO() #load the utility and its default values for now. We modify some args below\n",
    "optimizer.nmax_inner = 20 #the number of inner iterations\n",
    "optimizer.ppd_beta = 0.7 #this is explained later\n",
    "\n",
    "\n",
    "from LHS_sampler import generate_lhstrainingset\n",
    "generate_lhstrainingset(os.path.join(temp_d, 'samoo_runs', 'template'), seed=42, n_samples=swarm_size, n_dimensions=2, bounds=bounds)\n",
    "\n",
    "sys.path.remove(temp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bad53",
   "metadata": {},
   "source": [
    "Let's prepare all template files needed. We will generate three sets of template files:\n",
    "1. template_outer -- for the outer iteration\n",
    "2. template_inner -- for the inner iteration\n",
    "3. template_repo_update -- for updating the outer iteration repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join('fonseca_fleming_demo', 'samoo_runs'))\n",
    "optimizer.prep_templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1a603",
   "metadata": {},
   "source": [
    "Then, run a sweep of the initial training dataset through the complex model to get the deterministic output with which to train the GPR with.  Result of this will be saved in the directory \"Outer_0\". This is Step 2-3 of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.outer_sweep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outer(outer_dir, fig=None, ax=None, show=True):\n",
    "    if fig is None or ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if outer_dir.endswith('_0'):\n",
    "        outer_pareto = pd.read_csv(os.path.join(outer_dir, 'fon.pareto.summary.csv'))\n",
    "    else:\n",
    "        outer_pareto = pd.read_csv(os.path.join(outer_dir, 'outer_repo', 'outer_repo.pareto.summary.csv'))\n",
    "    \n",
    "    outer_pareto = outer_pareto.loc[outer_pareto['nsga2_front'] == 1]\n",
    "    \n",
    "    ax.scatter(outer_pareto['obj1'], outer_pareto['obj2'], c='none', ec='firebrick', s=20, zorder=10, label='Outer repository positions')\n",
    "    ax.scatter(true_pareto['obj1'], true_pareto['obj2'], c='deepskyblue', s=20, label='Pareto optimal solutions')\n",
    "    \n",
    "    ax.set_xlabel('Objective 1')\n",
    "    ax.set_ylabel('Objective 2')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_title(f'Outer-{outer_dir.split(\"_\")[1]}')\n",
    "    if show and fig is not None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax, outer_pareto\n",
    "\n",
    "plot_outer('outer_0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1dee3",
   "metadata": {},
   "source": [
    "Outer-0 produced some non-dominated positions. They are still far from the true Pareto optimal solutions.\n",
    "\n",
    "The input-output pair resulting from such sweep will be used as training dataset to retrain the GPR, which will be needed in the inner iterations. This GPR will be called in the inner iterations instead of the complex model.\n",
    "\n",
    "Let's prepare the GPR training files (Step 4) and proceed with the first inner iteration (Step 5), the output of which will be saved in \"Inner_1\" directory. We will do 20 inner iterations for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56261a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dirs, outer_dirs = optimizer.get_dirlist()\n",
    "optimizer.inner_prep(inner_dirs, outer_dirs) #this will write the training dataset\n",
    "next_inner_index = 1 if len(inner_dirs) == 0 else int(inner_dirs[-1].split(\"_\")[1]) + 1\n",
    "optimizer.inner_opt(next_inner_index) #this execute pestpp-mou that calls on the GPR in the inner iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5cb3c",
   "metadata": {},
   "source": [
    "At this stage the GPR only has very limited knowledge informed only by a sample of 30 points from the decision space and their corresponding deterministic objective function values. Thus, we recognize that GPR emulations are incorrect and these errors should be taken into consideration when evaluating dominance relationships in the inner iterations. SAMOO deals with these uncertainties through \"Probabilistic Pareto Dominance\" or PPD. The basic idea of PPD is that we cannot rule out dominated members to be completely suboptimal, likewise, we cannot trust that non-dominated members are truly Pareto optimal. Thus, if the dominance relationships does not reach our confidence threshold (passed through the ppd_beta argument in the pestpp-mou control file), we can just keep both solution in the 'surrogate repository' for now. As a result, we won't have a Pareto 'front' in the inner iterations but we will have as fuzzy set of solutions all of which could 'probably' be in the true front. We call this the \"Pareto cloud\". The cloudiness of the Pareto cloud is determined by the value of ppd_beta. In our experience, a ppd_beta between 0.5-0.7 generally works well in most cases. In this demo, we are using 0.6.\n",
    "\n",
    "Let's see what the Pareto cloud looks like through the inner iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dirs, outer_dirs = optimizer.get_dirlist()\n",
    "\n",
    "def plot_inner(inner_dir, iteration, ax = None, fig = None, show=True):\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    inner_rundata = pd.read_csv(os.path.join(inner_dir, 'fon.pareto.summary.csv'))\n",
    "    a = ax.scatter(true_pareto['obj1'], true_pareto['obj2'], c='deepskyblue', s = 20, label = 'Pareto optimal solutions')\n",
    "\n",
    "    pareto_cloud = inner_rundata.loc[(inner_rundata['generation'] == iteration) & (inner_rundata['nsga2_front'] == 1)]\n",
    "    b = ax.scatter(pareto_cloud['obj1'], pareto_cloud['obj2'], c='none', ec='g', s=30, label='Pareto cloud')\n",
    "\n",
    "    all_dv, all_obs = optimizer.parse_all_io(inner_dir)\n",
    "    pareto_dv = all_dv.loc[all_dv['real_name'].isin(pareto_cloud['member'])]\n",
    "\n",
    "    from matplotlib.patches import Ellipse\n",
    "    from matplotlib.patches import Patch\n",
    "    for mem in pareto_cloud['member']:\n",
    "        o1 = pareto_cloud.loc[pareto_cloud['member'] == mem, 'obj1'].item()\n",
    "        o2 = pareto_cloud.loc[pareto_cloud['member'] == mem, 'obj2'].item()\n",
    "        ellipse_ab = all_obs.loc[all_obs['real_name'] == mem]\n",
    "        o1sd = ellipse_ab['obj1_sd'].item()\n",
    "        o2sd = ellipse_ab['obj2_sd'].item()\n",
    "        ellipse = Ellipse((o1, o2), width=3*o1sd, height=3*o2sd, fc='peachpuff', zorder=-10)\n",
    "        ax.add_artist(ellipse)\n",
    "    \n",
    "    sdpatch = Patch(facecolor='peachpuff', edgecolor='none', label='$\\pm 3$ sd')\n",
    "    ax.legend(handles=[a, b, sdpatch], loc=0)\n",
    "    ax.set_xlabel('Objective 1')\n",
    "    ax.set_ylabel('Objective 2')\n",
    "    ax.set_title(f'Inner-{iteration}')\n",
    "\n",
    "    if show and fig is not None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax, pareto_dv, pareto_cloud\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def update_plots(iteration):\n",
    "    plt.close('all')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Pareto Cloud with uncertainty ellipse\n",
    "    _,_,pareto_dv,_ = plot_inner(inner_dirs[-1], iteration, ax = axes[0], fig = fig, show = False)\n",
    "\n",
    "    # Collect data for surface plots\n",
    "    training_dv = pd.read_csv(os.path.join(inner_dirs[-1], 'gp_0.dv_training.csv'))\n",
    "    training_obs = pd.read_csv(os.path.join(inner_dirs[-1], 'gp_0.obs_training.csv'))\n",
    "    training_data = pd.merge(training_dv, training_obs, on='real_name', how='inner')\n",
    "    \n",
    "    x1_train = training_data['x1'].values\n",
    "    x2_train = training_data['x2'].values\n",
    "    obj1_train = training_data['obj1'].values\n",
    "    obj2_train = training_data['obj2'].values\n",
    "    \n",
    "    grid_size = 100\n",
    "    x1_grid = np.linspace(-4, 4, grid_size)\n",
    "    x2_grid = np.linspace(-4, 4, grid_size)\n",
    "    X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
    "    \n",
    "    from scipy.interpolate import griddata\n",
    "    Z1 = griddata((x1_train, x2_train), obj1_train, (X1, X2), method='cubic')\n",
    "    Z2 = griddata((x1_train, x2_train), obj2_train, (X1, X2), method='cubic')\n",
    "    \n",
    "    # Plot 2: Objective 1 Surface\n",
    "    im1 = axes[1].contourf(X1, X2, Z1, cmap='plasma', levels=20)\n",
    "    plt.colorbar(im1, ax=axes[1], label='Objective 1')\n",
    "    axes[1].scatter(x1_train, x2_train, c='grey', ec='none', s=20, alpha=0.5, label='training points')\n",
    "    axes[1].scatter(pareto_dv['x1'], pareto_dv['x2'], c='none', ec='g', s=30, label='dv of Pareto cloud')\n",
    "    axes[1].set_xlabel('x1')\n",
    "    axes[1].set_ylabel('x2')\n",
    "    axes[1].set_title('Objective 1 Surface in Decision Space')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot 3: Objective 2 Surface\n",
    "    im2 = axes[2].contourf(X1, X2, Z2, cmap='plasma', levels=20)\n",
    "    plt.colorbar(im2, ax=axes[2], label='Objective 2')\n",
    "    axes[2].scatter(x1_train, x2_train, c='grey', ec='none', s=20, alpha=0.5, label='training points')\n",
    "    axes[2].scatter(pareto_dv['x1'], pareto_dv['x2'], c='none', ec='g', s=30, label='dv of Pareto cloud')\n",
    "    axes[2].set_xlabel('x1')\n",
    "    axes[2].set_ylabel('x2')\n",
    "    axes[2].set_title('Objective 2 Surface in Decision Space')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "slider = widgets.IntSlider(\n",
    "    value=optimizer.nmax_inner,\n",
    "    min=0,\n",
    "    max=optimizer.nmax_inner,\n",
    "    step=1,\n",
    "    description='Inner Iteration:',\n",
    "    continuous_update=False\n",
    ")\n",
    "widgets.interactive(update_plots, iteration=slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f907fd",
   "metadata": {},
   "source": [
    "Notice how the confidence ellipsoids of the Pareto cloud positions overlap significantly. If we simply follow the conventional Pareto dominance sorting, many of these positions would be dominated and discarded even if they may potentially be non-dominated considering their uncertainty. How many of these do you think will dominate those that are already in the repository from Outer-0?\n",
    "\n",
    "Also observe that the Pareto cloud cannot get close any further to the true Pareto front. This is a result of the limited knowledge of the GPR at this stage. We must therefore retrain the GPR to improve its prediction. The Pareto cloud contains the most informative solutions that, once evaluated through the complex model, are either nearly or already truly Pareto optimal solutions. We can therefore sample the Pareto cloud for decision variable values that are worth evaluating in the expensive complex model (also known as the infill points), which will then be added to the existing knowledge base (i.e., the training dataset) of the GPR and retrain the GPR with this updated information.\n",
    "\n",
    "Let's proceed with sampling this Pareto cloud for 30 infills -- this is Step 6 of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dirs, outer_dirs = optimizer.get_dirlist()\n",
    "optimizer.max_infill = swarm_size\n",
    "optimizer.resample(inner_dirs, outer_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c436a",
   "metadata": {},
   "source": [
    "We go back to Step 2-3 -- evaluate these infills through the complex model, which will be the Outer-1, and perform dominance evaluation of Pareto optimal solutions obtained in Outer-0 and Outer-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041c2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_dirs = optimizer.outer_sweep(1)\n",
    "optimizer.update_outer_repo(outer_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd0f69",
   "metadata": {},
   "source": [
    "Did we make some progress in finding better solutions than those currently in the repository from Outer-0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaafcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dirs, outer_dirs = optimizer.get_dirlist()\n",
    "\n",
    "def plot_comparison(outer_selection):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    if outer_selection == 'outer_0':\n",
    "        plot_outer('outer_0', ax=axes[1], fig=fig, show=False)\n",
    "        axes[0].set_title(\"No inner plot\")\n",
    "        axes[0].set_xlabel(\"Objective 1\")\n",
    "        axes[0].set_ylabel(\"Objective 2\")\n",
    "    else:\n",
    "        outer_num = outer_selection.split('_')[1]\n",
    "        matching_inner_dir = next((inner_dir for inner_dir in inner_dirs if inner_dir.endswith(f\"_{outer_num}\")), inner_dirs[-1])\n",
    "        _,_,_,pareto_cloud = plot_inner(matching_inner_dir, optimizer.nmax_inner, ax=axes[0], fig=fig, show=False)\n",
    "        _,_,outer_pareto = plot_outer(outer_selection, ax=axes[1], fig=fig, show=False)\n",
    "        \n",
    "        successful_infill = pareto_cloud.loc[pareto_cloud['member'].isin(outer_pareto['member'])]\n",
    "        axes[0].scatter(successful_infill['obj1'], successful_infill['obj2'], \n",
    "                       c='firebrick', ec='none', s=30, label='successful infill')\n",
    "        axes[0].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "outer_dropdown = widgets.Dropdown(\n",
    "    options=outer_dirs,\n",
    "    value='outer_1',\n",
    "    description='Outer Iteration:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "widgets.interactive(plot_comparison, outer_selection=outer_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3785a",
   "metadata": {},
   "source": [
    "That's definitely a lot of progress there as many of the infills selected are actually sitting at the true front even though the GPR prediction was incorrect. Also notice that some of the successful infill would have been discarded if we used the conventional Pareto dominance sorting and treat the GPR predicted positions as deterministic. We would have missed out really informative positions.\n",
    "\n",
    "Let's do 3 more outer iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba95246",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.restart = True #we will pickup from where we left off at Outer-1\n",
    "optimizer.nmax_outer = 3 #3 more outer iterations\n",
    "optimizer.run() #run the whole SAMOO process outlined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3942e42",
   "metadata": {},
   "source": [
    "Let's look at how we progressed in each outer iterations. Compare the resulting outer repository positions at each outer iteration. Notice how the confidence ellipsoid is reduced significantly in the inner iterations as more infills are added in the training dataset of the GPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dirs, outer_dirs = optimizer.get_dirlist()\n",
    "\n",
    "outer_dropdown = widgets.Dropdown(\n",
    "    options=outer_dirs,\n",
    "    value=outer_dirs[-1],\n",
    "    description='Outer Iteration:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "widgets.interactive(plot_comparison, outer_selection=outer_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce9027",
   "metadata": {},
   "source": [
    "Now, let's revisit the progress of the Complex MOO run that we did earlier. Since GPR emulations are way faster compared to the complex model, we can \"roughly\" compare the progress of the Complex MOO run and SAMOO by their repository positions after having run the same number of complex model evaluations -- that is, the regular iteration of complex MOO vs the outer iteration of MOO.\n",
    "\n",
    "Use the slider below to plot Iteration-1 of complex MOO and compare it with Outer-1 of SAMOO in the plots above. Do you see the difference? SAMOO is able to find objective positions that are near or already Pareto optimal with fewer complex model evaluations. Keep comparing other iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_slider = IntSlider(min=0, max=max_gen, step=1, value=max_gen, \n",
    "                             description='Generation:',\n",
    "                             continuous_update=False)\n",
    "moo_plot_all(max_gen)\n",
    "\n",
    "generation_slider.observe(lambda change: moo_plot_all(change['new']), names='value')\n",
    "display(VBox([\n",
    "    generation_slider,\n",
    "    HBox([out_obj_space, out_pareto]),\n",
    "    out_contour\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2280584",
   "metadata": {},
   "source": [
    "How many complex MOO iterations does it take to make it at the same level of progress that SAMOO had in Outer-1? Remember we are just pretending that the Fonseca-Fleming problem is numerically expensive and complex, so run times for both these algorithms don't differ much, but imagine how much computational budget you would have saved if the Fonseca-Fleming functions were real-world groundwater model with each forward run taking hours to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca0be0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Macasieb, R. Q., White, J. T., Pasetto, D., & Siade, A. J. (2025). A probabilistic approach to surrogate‐assisted multi‐objective optimization of complex groundwater problems (in production). Water Resources Research, 61, e2024WR038554. https://doi.org/10.1029/2024WR038554"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
